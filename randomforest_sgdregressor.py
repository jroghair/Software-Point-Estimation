# -*- coding: utf-8 -*-
"""RandomForest_SGDRegressor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rtgAdN2mxcGDq4ULrrY3tB0eeO4FnZ9g
"""

import os
import nltk
import re
from nltk import sent_tokenize
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Conv2D, Reshape, MaxPooling2D
from tensorflow.keras.layers import Embedding
## Plotly
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
import nltk
import string
import numpy as np
import pandas as pd
from nltk.corpus import stopwords
from sklearn.manifold import TSNE
import pandas as pd
print(tf.__version__)

# Import PyDrive and associated libraries.
# This only needs to be done once per notebook.
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client.
# This only needs to be done once per notebook.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Download a file based on its file ID.
#
# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz
file_id = '1FTXrKknj21DsW7KTmSPO5cQI1MAzPYMJ'
downloaded = drive.CreateFile({'id': file_id})
downloaded.GetContentFile('clean_moodle.csv')  
df = pd.read_csv('clean_moodle.csv')
#print('Downloaded content "{}"'.format(downloaded.GetContentString()))

df.head()

#one-hot encoding response variable
from sklearn.preprocessing import LabelBinarizer
lb = LabelBinarizer()
data['sp'] = lb.fit_transform(data['storypoint']).tolist()
data = data[['description', 'sp']]
data.head()

#change response to 3 categories
#1-4 = low
#5-13 = medium
#13+ = high
data = df.reset_index()[['description', 'storypoint']]
data.storypoint.value_counts()
#data.loc[data.storypoint <=4, 'low'] = 0

data.loc[(data.storypoint <= 4), 'storypoints_mod'] = 'low'
data.loc[(data.storypoint >13), 'storypoints_mod'] = 'high'
data.loc[((data.storypoint <=13) & (data.storypoint > 4)), 'storypoints_mod'] = 'medium'

#one hot encode again
data['sp'] = lb.fit_transform(data['storypoints_mod']).tolist()
data = data[['description', 'sp']]
data.head()

from sklearn.model_selection import train_test_split
from gensim.test.utils import common_texts
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from collections import namedtuple
from nltk import word_tokenize
nltk.download('punkt')

#data_sq.shape #1166,35
#df['storypoint'].shape #1166

#new_data = df.reset_index()[['description', 'storypoint']]
new_data = df.reset_index()[['description','storypoint']]
#new_data.storypoint.value_counts()
#new_data.loc[data.storypoint <=4, 'low'] = 0

new_data.loc[(new_data.storypoint <= 4), 'storypoint_mod'] = 0
new_data.loc[(new_data.storypoint >13), 'storypoint_mod'] = 2
new_data.loc[((new_data.storypoint <=13) & (new_data.storypoint > 4)), 'storypoint_mod'] = 1

#X_train, X_test, y_train, y_test = train_test_split(new_data['description'], new_data['storypoint_mod'], test_size=0.1, random_state=42)

doc1 = new_data['description'].to_list()
docs = []
analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')
for i, text in enumerate(doc1):
    words = text.lower().split()
    tags = [i]
    docs.append(analyzedDocument(words, tags))

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()),tags=[i]) for i, _d in enumerate(doc1)]

model = Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4)

#model.build_vocab(tagged_data)

max_epochs = 100
vec_size = 20
alpha = 0.025

for epoch in range(max_epochs):
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

X = model.docvecs.doctag_syn0

print(X.shape)

X_train, X_test, y_train, y_test = train_test_split(X, new_data['storypoint_mod'], test_size=0.1, random_state=42)

#SGDRegressor
from sklearn.linear_model import SGDRegressor
clf = SGDRegressor(max_iter=1000, tol=1e-3)
clf.fit(X_train, y_train)
predicted = clf.predict(X_test)
#df.head()
count = 0
for i in range(0, y_test.shape[0]):
  if y_test.iloc[i] == round(predicted[i]):
    count += 1

print("accuracy is: ", (count/y_test.shape[0]))
print(count)

#SGDClassifier
from sklearn.linear_model import SGDClassifier
clf = SGDClassifier(max_iter=1000, tol=1e-3)
clf.fit(X_train, y_train)
predicted = clf.predict(X_test)
#df.head()
count = 0
for i in range(0, y_test.shape[0]):
  if y_test.iloc[i] == predicted[i]:
    count += 1

print("accuracy is: ", (count/y_test.shape[0]))
print(count)

#Random Forest classifier
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=100)
rfc.fit(X_train, y_train)
#predicted = rfc.predict(X_test)
#df.head()
score = rfc.score(X_test, y_test)
print(score)

#Random Forest regressor
from sklearn.ensemble import RandomForestRegressor
rfr = RandomForestRegressor(n_estimators=100, max_depth=5)
rfr.fit(X_train, y_train)
predicted = rfr.predict(X_test)
#df.head()
count = 0
for i in range(0, y_test.shape[0]):
  if y_test.iloc[i] == round(predicted[i]):
    count += 1

print("accuracy is: ", (count/y_test.shape[0]))
print(count)

y_train

